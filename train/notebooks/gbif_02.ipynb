{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518a0c58",
   "metadata": {},
   "source": [
    "# Model Generation for GBIF Fungi Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411778a",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Transfer Learning with Hub](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)\n",
    "* [`tf.keras.utils.image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory)\n",
    "* [Limiting GPU Memory Growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285c8a1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c0f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from keras.utils.layer_utils import count_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a639289",
   "metadata": {},
   "source": [
    "### Limit GPU memory allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16845159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_memory_growth(limit=True):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, limit)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a47939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "limit_memory_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e1972",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8d406",
   "metadata": {},
   "source": [
    "## Enumerate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b652b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "flowers_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "flowers_data_dir = tf.keras.utils.get_file('flower_photos', origin=flowers_dataset_url, untar=True)\n",
    "flowers_data_dir = pathlib.Path(flowers_data_dir)\n",
    "\n",
    "datasets = [\n",
    "    ('CUB-200-2011', '/media/data/cub/CUB_200_2011/images'),\n",
    "    ('flowers', flowers_data_dir),\n",
    "    (\n",
    "        'GBIF_fungi',\n",
    "        '/media/data/gbif/media',\n",
    "        # '/mnt/gbif/media',\n",
    "        # (\n",
    "        #     '/mnt/gbif/clean_data.h5',\n",
    "        #     'media_merged_filtered-by-species_350pt',\n",
    "        #     'map',\n",
    "        #     'labels',\n",
    "        # ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9c943",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd46f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataset object from a directory of images\n",
    "def build_dataset(\n",
    "    dataset,\n",
    "    image_size,\n",
    "    preprocess_input = None,\n",
    "    batch_size = 64,\n",
    "):\n",
    "    folder_labels = True\n",
    "    labels = 'infer'\n",
    "    if len(dataset) > 2:\n",
    "        folder_labels = False\n",
    "        \n",
    "    if not folder_labels:\n",
    "        labels_df = pd.read_hdf(dataset[2][0], dataset[2][3])\n",
    "        labels = labels_df['acceptedScientificName'].tolist()\n",
    "        print(f'# Labels: {len(labels)}')\n",
    "   \n",
    "    train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dataset[1],\n",
    "        batch_size = batch_size,\n",
    "        validation_split = 0.05,\n",
    "        image_size = image_size,\n",
    "        subset = \"both\",\n",
    "        shuffle = True, # default but here for clarity\n",
    "        seed = 42,\n",
    "        label_mode = \"categorical\", # enables one-hot encoding (use 'int' for sparse_categorical_crossentropy loss)\n",
    "        # labels = labels, # need to default to 'infer', not None\n",
    "    )\n",
    "    \n",
    "    # Retrieve class names\n",
    "    # (can't do this after converting to PrefetchDataset?)\n",
    "    class_names = train_ds.class_names\n",
    "    \n",
    "    # Prefetch images\n",
    "    # train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    # val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # apply preprocessing function\n",
    "    train_ds = train_ds.map(\n",
    "        lambda x, y: (preprocess_input(x), y),\n",
    "        num_parallel_calls = 16,\n",
    "    )\n",
    "    val_ds = val_ds.map(\n",
    "        lambda x, y: (preprocess_input(x), y),\n",
    "        num_parallel_calls = 16,\n",
    "    )\n",
    "    \n",
    "    return (train_ds, val_ds, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de845ab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf56f3",
   "metadata": {},
   "source": [
    "## Enumerate Models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee979c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of tuples describing the models to be tested\n",
    "# in the form: (model_handle, input_image_size, preprocessing_function)\n",
    "# where the model_handle is a model building function or a url to a tfhub feature model\n",
    "base_models_metadata = [\n",
    "    (\n",
    "        'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4',\n",
    "        224,\n",
    "        # https://www.tensorflow.org/hub/common_signatures/images#input\n",
    "        # The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "        tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        'https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4',\n",
    "        299,\n",
    "        # The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "        tf.keras.applications.inception_v3.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        'https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/5',\n",
    "        299,\n",
    "        # The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "        tf.keras.applications.inception_v3.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        tf.keras.applications.Xception,\n",
    "        299,\n",
    "        # The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "        tf.keras.applications.xception.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        tf.keras.applications.resnet.ResNet101,\n",
    "        224,\n",
    "        tf.keras.applications.resnet50.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        tf.keras.applications.ResNet50,\n",
    "        224,\n",
    "        tf.keras.applications.resnet50.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        tf.keras.applications.InceptionResNetV2,\n",
    "        299,\n",
    "        tf.keras.applications.inception_resnet_v2.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        tf.keras.applications.efficientnet_v2.EfficientNetV2B0,\n",
    "        224,\n",
    "        # The preprocessing logic has been included in the EfficientNetV2\n",
    "        # model implementation. Users are no longer required to call this\n",
    "        # method to normalize the input data. This method does nothing and\n",
    "        # only kept as a placeholder to align the API surface between old\n",
    "        # and new version of model.\n",
    "        tf.keras.applications.efficientnet_v2.preprocess_input,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a0c6376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a name that accurately describes the model building function or\n",
    "# the tfhub model (by url) that was passed\n",
    "def get_model_name( model_handle ):\n",
    "    \n",
    "    if callable(model_handle):\n",
    "        return f'keras.applications/{model_handle.__name__}'\n",
    "    else:\n",
    "        split = model_handle.split('/')\n",
    "        return f'tfhub/{split[-5]}.{split[-4]}.{split[-3]}'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa7788",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06cf38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FullModel(tf.keras.Sequential):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_metadata,\n",
    "        num_classes,\n",
    "        dropout,\n",
    "        thawed_base_model_layers,\n",
    "        data_augmentation = False,\n",
    "    ):\n",
    "        super().__init__(name = \"full_model\")\n",
    "        \n",
    "        # Get base_model_information\n",
    "        model_handle, input_dimension, preprocess_input = base_model_metadata\n",
    "        print(input_dimension)\n",
    "        \n",
    "        # TODO: only really needed on training data...?\n",
    "        #  - should be a function of the dataset, no?\n",
    "        # if data_augmentation:\n",
    "        #     self.add(self.build_data_augmentation())\n",
    "        \n",
    "        self.base_model = self.build_base_model(\n",
    "            model_handle,\n",
    "            input_shape=(input_dimension, input_dimension) + (3,),\n",
    "            thawed_base_model_layers = thawed_base_model_layers,\n",
    "        )\n",
    "        self.add(self.base_model)\n",
    "\n",
    "        self.classifier_model = self.build_classifier_model(\n",
    "            num_classes,\n",
    "            dropout,\n",
    "        )\n",
    "        self.add(self.classifier_model)\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_data_augmentation():\n",
    "        data_augmentation = keras.Sequential([\n",
    "            layers.RandomFlip(\n",
    "                \"horizontal\",\n",
    "            ),\n",
    "            layers.RandomRotation(0.1),\n",
    "            layers.RandomZoom(0.1),\n",
    "        ], name = \"data_augmentation\")\n",
    "\n",
    "        return data_augmentation\n",
    "\n",
    "    @staticmethod\n",
    "    def build_base_model(\n",
    "        base_model_handle,\n",
    "        input_shape,\n",
    "        thawed_base_model_layers = 0,\n",
    "        name=\"base_model\",\n",
    "    ):\n",
    "        # If model_handle is a model building function, use that function\n",
    "        if callable(base_model_handle):\n",
    "            base_model = model_handle(\n",
    "                include_top=False,\n",
    "                input_shape=input_shape,\n",
    "                weights='imagenet',\n",
    "                pooling = 'avg',\n",
    "            )\n",
    "\n",
    "        # otherwise build a layer from the tfhub url that was passed as a string\n",
    "        else:\n",
    "            base_model = hub.KerasLayer(\n",
    "                base_model_handle,\n",
    "                input_shape=input_shape,\n",
    "                name=name,\n",
    "            )\n",
    "            \n",
    "        # Freeze specified # of layers\n",
    "        # FullModel.freeze_base_model(base_model, thawed_base_model_layers)\n",
    "        base_model.trainable = True\n",
    "\n",
    "\n",
    "        # Print Base model weights\n",
    "        print(\"\\nBase Model:\")\n",
    "        FullModel.print_weight_counts(base_model)\n",
    "\n",
    "        return base_model\n",
    "    \n",
    "    # Freeze base model?\n",
    "    @staticmethod\n",
    "    def freeze_base_model(\n",
    "        base_model,\n",
    "        thawed_base_model_layers = 0,\n",
    "    ):\n",
    "        print(\"Thawing...\", thawed_base_model_layers)\n",
    "        \n",
    "        print(base_model)\n",
    "        print(base_model.summary())\n",
    "        \n",
    "        if thawed_base_model_layers == 0:\n",
    "            base_model.trainable = False\n",
    "        elif thawed_base_model_layers > 0:\n",
    "            for layer in base_model.layers[(-1*thawed_base_model_layers):]:\n",
    "                layer.trainable = False\n",
    "        else:\n",
    "            base_model.trainable = True\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_classifier_model(\n",
    "        num_classes,\n",
    "        dropout,\n",
    "    ):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                num_classes,\n",
    "                # activation = 'softmax',\n",
    "            )\n",
    "        )\n",
    "\n",
    "        model.add(\n",
    "            layers.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        model.add(\n",
    "            layers.Activation(\"softmax\", dtype=\"float32\")\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # Print model weight counts\n",
    "    @staticmethod\n",
    "    def print_weight_counts(model):\n",
    "        print(f'Non-trainable weights: {count_params(model.non_trainable_weights)}')\n",
    "        print(f'Trainable weights: {count_params(model.trainable_weights)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65bf126",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d60b33",
   "metadata": {},
   "source": [
    "## Run Results Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3401105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunLogging():\n",
    "\n",
    "    hdf_key = \"runs\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir = \"./\",\n",
    "        hdf_filename = \"runs.h5\",\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        self.filename =  os.path.join( data_dir, hdf_filename )\n",
    "        self.df = self.load_metadata()\n",
    "\n",
    "    def load_metadata(self):\n",
    "        if ( os.path.exists(self.filename) ):\n",
    "            return pd.read_hdf( self.filename, self.hdf_key )\n",
    "        else: return pd.DataFrame()\n",
    "\n",
    "    def save_df(self):\n",
    "        self.df.to_hdf(self.filename, self.hdf_key)\n",
    "\n",
    "    def add_run(self, params, log_dir, time, scores):\n",
    "\n",
    "        cols = {\n",
    "            **params,\n",
    "            'time': time,\n",
    "            'log_dir': log_dir,\n",
    "            'scores.loss': scores[0],\n",
    "            'scores.accuracy': scores[1],\n",
    "            'scores.top3': scores[2],\n",
    "            'scores.top10': scores[3],\n",
    "        }\n",
    "\n",
    "        new_run = pd.DataFrame([cols])\n",
    "\n",
    "        self.df = pd.concat([self.df, new_run])\n",
    "        self.save_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6dd65",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4286d70",
   "metadata": {},
   "source": [
    "## Build and run all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b6575",
   "metadata": {},
   "source": [
    "* Note regarding `thawed_base_model_layers` and full model architecture ([reference](https://stackoverflow.com/questions/64227483/what-is-the-right-way-to-gradually-unfreeze-layers-in-neural-network-while-learn))\n",
    "![image](https://i.stack.imgur.com/JLJqv.png)\n",
    "* [Another great reference](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed107505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "## Dataset Hyperparameters\n",
    "dataset = datasets[2] # GBIF_fungi dataset\n",
    "batch_size = 64\n",
    "\n",
    "## Full Model Hyperparameters\n",
    "\n",
    "### Input Model Hyperparameters\n",
    "data_augmentation = False\n",
    "\n",
    "### Base Model Hyperparameters\n",
    "# thawed_base_model_layers = 0 # base_model completely frozen\n",
    "# thawed_base_model_layers = n # last 'n' layers of base_model unfrozen\n",
    "thawed_base_model_layers = -1 # all base_model layers unfrozen\n",
    "\n",
    "### Classifier Model Hyperparameters\n",
    "classifier_dropout = 0.33\n",
    "\n",
    "## Training Hyperparameters\n",
    "max_epochs = 20\n",
    "\n",
    "### Optimizer Function Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "\n",
    "### Loss function hyperparameters\n",
    "label_smoothing = 0.1\n",
    "\n",
    "# use logits?\n",
    "\n",
    "# vary classifier architecture?\n",
    "\n",
    "\n",
    "load_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81cb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_name(name):\n",
    "    \n",
    "    split = name.split('.')\n",
    "    \n",
    "    return {\n",
    "        \"index\": int(split[0]),\n",
    "        \"name\": split[1].replace('_', ' '),\n",
    "    }\n",
    "\n",
    "def save_classnames(class_names, log_dir):\n",
    "    df = pd.DataFrame(list(map(parse_name, class_names)))\n",
    "    mapping = {}\n",
    "    l = list(map(parse_name, class_names))\n",
    "    for item in l:\n",
    "        mapping[item['index']] = item['name']\n",
    "    with open( os.path.join( log_dir, 'classes.json'), 'w' )  as out:\n",
    "        json.dump(mapping, out, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03b3f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logging\n",
    "run_logs = RunLogging(\n",
    "    data_dir = f'models_cub_02_logs',\n",
    ")\n",
    "\n",
    "# for each base model\n",
    "#for base_model_metadata in base_models_metadata:\n",
    "base_model_metadata = base_models_metadata[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab9e4dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 665803 files belonging to 2451 classes.\n",
      "Using 632513 files for training.\n",
      "Using 33290 files for validation.\n",
      "WARNING:tensorflow:From /home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "299\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "\n",
      "Base Model:\n",
      "Non-trainable weights: 34432\n",
      "Trainable weights: 21768352\n",
      "\n",
      "models_cub_02_logs/GBIF_fungi/tfhub/google.inaturalist.inception_v3\n",
      "Epoch 1/20\n",
      "2318/9884 [======>.......................] - ETA: 34:05 - loss: 7.2210 - accuracy: 0.0542 - auc: 0.5813 - categorical_crossentropy: 6.7629 - Top3: 0.1048 - Top10: 0.1832"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_handle, input_dimension, preprocess_input = base_model_metadata\n",
    "\n",
    "image_size = (input_dimension, input_dimension)\n",
    "\n",
    "# Build dataset/pipeline\n",
    "train_ds, val_ds, class_names = build_dataset(\n",
    "    dataset,\n",
    "    batch_size = batch_size,\n",
    "    image_size = image_size,\n",
    "    preprocess_input = preprocess_input,\n",
    ")\n",
    "\n",
    "def build_new_model():\n",
    "    # Build model\n",
    "    model = FullModel(\n",
    "        base_model_metadata,\n",
    "        len(class_names),\n",
    "        classifier_dropout,\n",
    "        thawed_base_model_layers = thawed_base_model_layers,\n",
    "        data_augmentation = data_augmentation,\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    # Sparse vs non-sparse CCE https://www.kaggle.com/general/197993\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "            # from_logits=True,\n",
    "            label_smoothing = label_smoothing,\n",
    "        ),\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.AUC(),\n",
    "            tf.keras.metrics.CategoricalCrossentropy(),            \n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(k=3, name=\"Top3\"),\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(k=10, name=\"Top10\"),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_model_id = get_model_name(model_handle)\n",
    "run_log_dir = os.path.join( run_logs.data_dir, dataset[0], base_model_id )\n",
    "\n",
    "saved_model_dir = os.path.join(run_log_dir + '.03', 'best_model')\n",
    "\n",
    "if os.path.isdir(saved_model_dir):\n",
    "    print(saved_model_dir)\n",
    "    print(\"Saved Model Exists, loading.\")\n",
    "    print(saved_model_dir)\n",
    "    old_model = tf.keras.models.load_model(saved_model_dir)\n",
    "    if ( load_weights ):\n",
    "        print(\"Loading weights from saved model\")\n",
    "        model = build_new_model()\n",
    "        model.load_weights(saved_model_dir)\n",
    "    else:\n",
    "        model = old_model\n",
    "else:\n",
    "    model = build_new_model()\n",
    "\n",
    "# Logging\n",
    "print(f'\\n{run_log_dir}')\n",
    "\n",
    "# Tensorboard logs\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir = run_log_dir,\n",
    "    histogram_freq = 1,\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor='val_sparse_categorical_accuracy',\n",
    "    monitor = 'val_loss',\n",
    "    patience = 5,\n",
    "    min_delta = 0.01,\n",
    ")\n",
    "\n",
    "# Model Checkpoints for saving best model weights\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(run_log_dir, 'best_model' ),\n",
    "    save_best_only = True,\n",
    "    monitor = 'val_loss',\n",
    "    # mode = 'min', # should be chosen correctly based on monitor value\n",
    ")\n",
    "\n",
    "best_batch = (0, float('inf'))\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=max_epochs,\n",
    "    callbacks=[\n",
    "        tensorboard_callback,\n",
    "        # early_stopping_callback,\n",
    "        model_checkpoint_callback,\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end = lambda batch, logs: print((batch, logs['loss']) if logs['loss'] < best_batch[1] else best_batch),\n",
    "        )\n",
    "    ],\n",
    "    # validation_freq=2,\n",
    ")\n",
    "\n",
    "print(history)\n",
    "print(max(history.history['val_loss']))\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "save_classnames(class_names, run_log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ddc2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"dataset\": dataset[0],\n",
    "    \"base_model\": 'xception',\n",
    "    \"batch_size\": batch_size,\n",
    "    \"data_augmentation\": data_augmentation,\n",
    "    \"thawed_base_model_layers\": thawed_base_model_layers,\n",
    "    \"classifier_dropout\": classifier_dropout,\n",
    "    \"max_epochs\": max_epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"label_smoothing\": label_smoothing,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dbe1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in hparams.items():\n",
    "    print(k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00575ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hparams(hparams, log_dir):\n",
    "    with open( os.path.join( log_dir, 'hparams.json'), 'w' )  as out:\n",
    "        json.dump(hparams, out, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hparams(hparams, run_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3fbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac2f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fungi]",
   "language": "python",
   "name": "conda-env-.conda-fungi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
